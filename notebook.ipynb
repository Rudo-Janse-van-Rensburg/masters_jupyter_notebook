{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d5620b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate in /home/rjansevanrensburg/.local/lib/python3.9/site-packages (0.3.0)\n",
      "Requirement already satisfied: transformers in /home/rjansevanrensburg/.local/lib/python3.9/site-packages (4.10.3)\n",
      "Requirement already satisfied: nlp in /home/rjansevanrensburg/.local/lib/python3.9/site-packages (0.4.0)\n",
      "Requirement already satisfied: datasets in /home/rjansevanrensburg/.local/lib/python3.9/site-packages (1.9.0)\n",
      "Requirement already satisfied: torch==1.13.0 in /home/rjansevanrensburg/.local/lib/python3.9/site-packages (1.13.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/rjansevanrensburg/.local/lib/python3.9/site-packages (from torch==1.13.0) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /apps/chpc/chem/anaconda3-2021.11/lib/python3.9/site-packages (from torch==1.13.0) (3.10.0.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/rjansevanrensburg/.local/lib/python3.9/site-packages (from torch==1.13.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/rjansevanrensburg/.local/lib/python3.9/site-packages (from torch==1.13.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/rjansevanrensburg/.local/lib/python3.9/site-packages (from torch==1.13.0) (11.7.99)\n",
      "Requirement already satisfied: wheel in /apps/chpc/chem/anaconda3-2021.11/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (0.37.0)\n",
      "Requirement already satisfied: setuptools in /apps/chpc/chem/anaconda3-2021.11/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (58.0.4)\n",
      "Requirement already satisfied: pyaml>=20.4.0 in /home/rjansevanrensburg/.local/lib/python3.9/site-packages (from accelerate) (21.10.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/rjansevanrensburg/.local/lib/python3.9/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: packaging in /apps/chpc/chem/anaconda3-2021.11/lib/python3.9/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.12 in /home/rjansevanrensburg/.local/lib/python3.9/site-packages (from transformers) (0.0.19)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /apps/chpc/chem/anaconda3-2021.11/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: sacremoses in /home/rjansevanrensburg/.local/lib/python3.9/site-packages (from transformers) (0.0.53)\n",
      "Requirement already satisfied: filelock in /apps/chpc/chem/anaconda3-2021.11/lib/python3.9/site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /apps/chpc/chem/anaconda3-2021.11/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: requests in /apps/chpc/chem/anaconda3-2021.11/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /apps/chpc/chem/anaconda3-2021.11/lib/python3.9/site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /apps/chpc/chem/anaconda3-2021.11/lib/python3.9/site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: pyarrow>=0.16.0 in /home/rjansevanrensburg/.local/lib/python3.9/site-packages (from nlp) (10.0.0)\n",
      "Requirement already satisfied: dill in /home/rjansevanrensburg/.local/lib/python3.9/site-packages (from nlp) (0.3.6)\n",
      "Requirement already satisfied: xxhash in /home/rjansevanrensburg/.local/lib/python3.9/site-packages (from nlp) (3.1.0)\n",
      "Requirement already satisfied: pandas in /apps/chpc/chem/anaconda3-2021.11/lib/python3.9/site-packages (from nlp) (1.3.4)\n",
      "Requirement already satisfied: multiprocess in /home/rjansevanrensburg/.local/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/rjansevanrensburg/.local/lib/python3.9/site-packages (from datasets) (2022.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /apps/chpc/chem/anaconda3-2021.11/lib/python3.9/site-packages (from packaging->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /apps/chpc/chem/anaconda3-2021.11/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /apps/chpc/chem/anaconda3-2021.11/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /apps/chpc/chem/anaconda3-2021.11/lib/python3.9/site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /apps/chpc/chem/anaconda3-2021.11/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /apps/chpc/chem/anaconda3-2021.11/lib/python3.9/site-packages (from pandas->nlp) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /apps/chpc/chem/anaconda3-2021.11/lib/python3.9/site-packages (from pandas->nlp) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /apps/chpc/chem/anaconda3-2021.11/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->nlp) (1.16.0)\n",
      "Requirement already satisfied: click in /apps/chpc/chem/anaconda3-2021.11/lib/python3.9/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: joblib in /apps/chpc/chem/anaconda3-2021.11/lib/python3.9/site-packages (from sacremoses->transformers) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "#We will be using packages: transformers, torch, nlp, datasets\n",
    "#!pip install accelerate==0.3.0 transformers==4.10.3 nlp==0.4.0 datasets==1.9.0 torch==1.13.0\n",
    "!pip install accelerate transformers nlp datasets torch==1.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a0da2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import datasets\n",
    "from datasets import load_dataset, dataset_dict\n",
    "import nlp\n",
    "import dataclasses\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "207022cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch \n",
    "import torch\n",
    "import torch.nn  as nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "#from transformers.training_args import is_tpu_available\n",
    "#from transformers.trainer.file_utils import  is_torch_tpu_available as is_tpu_available\n",
    "#from transformers.trainer import get_tpu_sampler\n",
    "from transformers import DataCollator\n",
    "from transformers.data.data_collator import InputDataClass\n",
    "#from transformers.data.data_collator import DataCollator, InputDataClass\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "import torch_xla\n",
    "#import torch_xla.debug.profiler as xp\n",
    "#import torch_xla.utils.keyd_queue as kq\n",
    "#import torch_xla.utils.utils as xu\n",
    "#import torch_xla.core.xla_model as xm\n",
    "from typing import List, Union, Dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01b665e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54371684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/rjansevanrensburg/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Using custom data configuration commonsense_qa\n",
      "Reusing dataset commonsense_qa (/home/rjansevanrensburg/.cache/huggingface/datasets/commonsense_qa/commonsense_qa/0.1.0/1ca2d7b680c5bd93c0dc85f9cb65c0c8817e759ff82e405b28de54e83efa80f7)\n",
      "Reusing dataset stsb_multi_mt (/home/rjansevanrensburg/.cache/huggingface/datasets/stsb_multi_mt/en/1.0.0/bc6de0eaa8d97c28a4c22a07e851b05879ae62c60b0b69dd6b331339e8020f07)\n"
     ]
    }
   ],
   "source": [
    "#Loading datasets \n",
    "dataset_dict = {\n",
    "    \"mnli\": datasets.load_dataset('glue', 'mnli'),\n",
    "    \"commonsense_qa\": datasets.load_dataset('commonsense_qa',name='commonsense_qa'),\n",
    "    \"stsb_multi_mt\": datasets.load_dataset(\"stsb_multi_mt\",name=\"en\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b96a34b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task: mnli\n",
      "{'premise': 'One of our number will carry out your instructions minutely.', 'hypothesis': 'A member of my team will execute your orders with immense precision.', 'label': 0, 'idx': 2}\n",
      "\n",
      "\n",
      "task: commonsense_qa\n",
      "{'answerKey': 'A', 'question': 'To locate a choker not located in a jewelry box or boutique where would you go?', 'choices': {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['jewelry store', 'neck', 'jewlery box', 'jewelry box', 'boutique']}}\n",
      "\n",
      "\n",
      "task: stsb_multi_mt\n",
      "{'sentence1': 'A man is spreading shreded cheese on a pizza.', 'sentence2': 'A man is spreading shredded cheese on an uncooked pizza.', 'similarity_score': 3.799999952316284}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, value in  dataset_dict.items():\n",
    "    print(\"task:\", key)\n",
    "    print(dataset_dict[key]['train'][2])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc0fa38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskModel(transformers.PreTrainedModel):\n",
    "\n",
    "    def __init__(self, encoder, taskmodels_dict):\n",
    "        \n",
    "        \"\"\"\n",
    "        Setting MultitaskModel up as a PretrainedModel allows us\n",
    "        to take better advantage of Trainer features\n",
    "        \"\"\"\n",
    "        super().__init__(transformers.PretrainedConfig())\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.taskmodels_dict = nn.ModuleDict(taskmodels_dict)\n",
    "    \n",
    "    @classmethod\n",
    "    def create_model(cls, model_name, model_type_dict, model_config_dict):\n",
    "        \"\"\"\n",
    "        This creates a MultitaskModel using the model class and config objects\n",
    "        from single-task models. \n",
    "\n",
    "        We do this by creating each single-task model, and having them share\n",
    "        the same encoder transformer.\n",
    "        \"\"\"\n",
    "        shared_encoder = None\n",
    "        task_models_dict = {}\n",
    "        for task, model_type in model_type_dict.items():\n",
    "            print(task)\n",
    "            print(model_type)\n",
    "            model = model_type.from_pretrained(\n",
    "                model_name, \n",
    "                config=model_config_dict[task],\n",
    "            )\n",
    "            if shared_encoder is None:\n",
    "                shared_encoder = getattr(model, cls.get_encoder_attr_name(model))\n",
    "            else:\n",
    "                setattr(model, cls.get_encoder_attr_name(model), shared_encoder)\n",
    "            task_models_dict[task] = model\n",
    "        return cls(encoder=shared_encoder, taskmodels_dict=task_models_dict)\n",
    "    \n",
    "        \n",
    "    @classmethod              \n",
    "    def get_encoder_attr_name(cls, model):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Each encoder has its attributes according to model architecture: BERT, Roberta,Alberta \n",
    "        This function gets attribute of the encoder.\n",
    "        \"\"\"\n",
    "        model_class_name = model.__class__.__name__\n",
    "        if model_class_name.startswith('Bert'):\n",
    "            return 'bert'\n",
    "        if model_class_name.startswith('Roberta'):\n",
    "            return 'roberta'\n",
    "        if model_class_name.startswith('Albert'):\n",
    "            return 'albert'\n",
    "        if model_class_name.startswith():\n",
    "            return \n",
    "            \n",
    "        else:\n",
    "                raise KeyError(f\"Add support for new model {model_class_name}\")\n",
    "                \n",
    "        \n",
    "    def forward(self, task, **kwargs):\n",
    "        return self.taskmodels_dict[task](**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58c45128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnli\n",
      "<class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "396ea064cbb84299af64e959a3ead76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commonsense_qa\n",
      "<class 'transformers.models.auto.modeling_auto.AutoModelForMultipleChoice'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stsb_multi_mt\n",
      "<class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "multitask_model=MultitaskModel.create_model(model_name=model_name, \n",
    "         model_type_dict={\n",
    "                 \"mnli\" : transformers.AutoModelForSequenceClassification,\n",
    "                 \"commonsense_qa\": transformers.AutoModelForMultipleChoice,\n",
    "                 \"stsb_multi_mt\"          : transformers.AutoModelForSequenceClassification\n",
    "         },\n",
    "         model_config_dict={\n",
    "                 \"mnli\" : transformers.AutoConfig.from_pretrained(model_name,num_labels=1),\n",
    "                 \"commonsense_qa\": transformers.AutoConfig.from_pretrained(model_name),\n",
    "                 \"stsb_multi_mt\"          : transformers.AutoConfig.from_pretrained(model_name,num_labels=1)\n",
    "             \n",
    "         }                                   \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e16aa927",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=transformers.AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faac9cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function convert_to_mnli_inputs at 0x7fffe60a5790> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------task---------: mnli\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78bd5a9f4914221b6134d67093a8660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/393 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjansevanrensburg/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2198: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnli train 392702 392702\n",
      "mnli train 392702 392702\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95976d06d6db4204a297258d1d681f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnli validation_matched 9815 9815\n",
      "mnli validation_matched 9815 9815\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d9195745a84e9c92b68c542093caba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnli validation_mismatched 9832 9832\n",
      "mnli validation_mismatched 9832 9832\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3d48bad1324f988bce8b5e5ea611c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnli test_matched 9796 9796\n",
      "mnli test_matched 9796 9796\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78818edea366415d924aecb2140a2876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnli test_mismatched 9847 9847\n",
      "mnli test_mismatched 9847 9847\n",
      "--------------task---------: commonsense_qa\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292b9891c78543edb2cbac141516ec8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commonsense_qa train 9741 9741\n",
      "commonsense_qa train 9741 9741\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6870fc281f0344139711c2a73c30f20f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commonsense_qa validation 1221 1221\n",
      "commonsense_qa validation 1221 1221\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ab49fdefaf41ee9b290f4bf0c0b0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commonsense_qa test 1140 1140\n",
      "commonsense_qa test 1140 1140\n",
      "--------------task---------: stsb_multi_mt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca93bd8433648e8a370e05b5565c03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stsb_multi_mt train 5749 5749\n",
      "stsb_multi_mt train 5749 5749\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8302d3c45f4267b115673511364abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stsb_multi_mt test 1379 1379\n",
      "stsb_multi_mt test 1379 1379\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e704adbf6d024c3b9e572b1989d704b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stsb_multi_mt dev 1500 1500\n",
      "stsb_multi_mt dev 1500 1500\n"
     ]
    }
   ],
   "source": [
    "max_length=256\n",
    "def convert_to_mnli_inputs(example_batch):\n",
    "    \n",
    "    inputs=list(zip(example_batch['premise'],example_batch['hypothesis']))\n",
    "    features=tokenizer.batch_encode_plus(inputs, max_length=max_length, \n",
    "                                         pad_to_max_length=True,truncation=True)\n",
    "    features['labels']=example_batch['label']\n",
    "    return features\n",
    "\n",
    "def convert_to_stsb_multi_mt_inputs(example_batch):\n",
    "    \n",
    "    inputs=list(zip(example_batch['sentence1'],example_batch['sentence2']))\n",
    "    features=tokenizer.batch_encode_plus(inputs, max_length=max_length, \n",
    "                                         pad_to_max_length=True,truncation=True)\n",
    "    features['labels']=example_batch['similarity_score']\n",
    "    return features\n",
    "\n",
    "def convert_to_commonsense_qa_inputs(example_batch):\n",
    "    \n",
    "    number_examples=len(example_batch['question'])\n",
    "    number_choices=len(example_batch['choices'][0]['text'])\n",
    "    features={}\n",
    "    for example in range(number_examples):\n",
    "        choices=tokenizer.batch_encode_plus(\n",
    "            list(zip([example_batch['question'][example]]*number_choices,\n",
    "                        example_batch['choices'][example]['text'])),\n",
    "            max_length=max_length, pad_to_max_length=True ,truncation=True\n",
    "                    )\n",
    "        for k,v in choices.items():\n",
    "            if k not in  features:\n",
    "                features[k]=[]\n",
    "            features[k].append(v)\n",
    "    labels2id={char: x for x, char in enumerate('ABCDE')}\n",
    "    if example_batch['answerKey'][0]:\n",
    "        features['labels']= [labels2id[ans] for ans in example_batch['answerKey']]\n",
    "    else:\n",
    "        features['labels']=[0]*number_examples\n",
    "    return features\n",
    "\n",
    "#Construct the featurized input data\n",
    "featurized_funct_dict={\n",
    "                 \"mnli\"          : convert_to_mnli_inputs,\n",
    "                 \"commonsense_qa\": convert_to_commonsense_qa_inputs,\n",
    "                 \"stsb_multi_mt\" : convert_to_stsb_multi_mt_inputs\n",
    "}\n",
    "\n",
    "column_dict={\n",
    "                 \"mnli\"          : ['input_ids', 'attention_mask','labels'],\n",
    "                 \"commonsense_qa\": ['input_ids', 'attention_mask','labels'],\n",
    "                 \"stsb_multi_mt\" : ['input_ids', 'attention_mask','labels']\n",
    "}\n",
    "#Featurizing datasets\n",
    "features_dict={}\n",
    "for  task, dataset in dataset_dict.items():\n",
    "    print(\"--------------task---------:\",task)\n",
    "    features_dict[task]={}\n",
    "    for phase, phase_dataset in dataset.items():\n",
    "       \n",
    "        features_dict[task][phase]=phase_dataset.map(featurized_funct_dict[task],\n",
    "                                                     batched=True, \n",
    "                                                     load_from_cache_file=False)\n",
    "        print(task, phase, len(phase_dataset), len(features_dict[task][phase]))\n",
    "        features_dict[task][phase].set_format(\n",
    "            type='torch',\n",
    "            columns=column_dict[task]\n",
    "        )\n",
    "        print(task, phase, len(phase_dataset), len(features_dict[task][phase]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09837ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrIgnoreDevice(str):\n",
    "    \"\"\"\n",
    "    This is a hack. The Trainer is going call .to(device) on every input\n",
    "    value, but we need to pass in an additional `task_name` string.\n",
    "    This prevents it from throwing an error\n",
    "    \"\"\"\n",
    "    def to(self, device):\n",
    "        return self        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4a8c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPDataCollator:#(DataCollator):\n",
    "    def collate_batch(self, features:List[Union[InputDataClass,Dict]]) -> Dict[str,torch.Tensor]:\n",
    "        first=features[0]\n",
    "        if isinstance(first,dict):\n",
    "            \n",
    "            #featurized dataset are in the form of list of dictionaries\n",
    "            #Adapt the DataCollator to have a list of dictionary\n",
    "            if \"labels\" in first and first[\"labels\"] is not None:\n",
    "                if first[\"labels\"].dtype == torch.int64:\n",
    "                    labels = torch.tensor(\n",
    "                        [f[\"labels\"] for f in features], dtype=torch.float\n",
    "                    )\n",
    "                else:\n",
    "                    labels = torch.tensor(\n",
    "                        [f[\"labels\"] for f in features], dtype=torch.float\n",
    "                    )\n",
    "                batch = {\"labels\": labels}\n",
    "            for k, v in first.items():\n",
    "                if k != \"labels\" and v is not None and not isinstance(v, str):\n",
    "                    batch[k] = torch.stack([f[k] for f in features])\n",
    "            return batch\n",
    "        else:\n",
    "            # otherwise, revert to using the default collate_batch\n",
    "            return DefaultDataCollator().collate_batch(features)\n",
    "        \n",
    "    def __call__(self, features:List[Union[InputDataClass,Dict]]) -> Dict[str,torch.Tensor]:\n",
    "        first=features[0]\n",
    "        if isinstance(first,dict):\n",
    "            \n",
    "            #featurized dataset are in the form of list of dictionaries\n",
    "            #Adapt the DataCollator to have a list of dictionary\n",
    "            if \"labels\" in first and first[\"labels\"] is not None:\n",
    "                if first[\"labels\"].dtype == torch.int64:\n",
    "                    labels = torch.tensor(\n",
    "                        [f[\"labels\"] for f in features], dtype=torch.float\n",
    "                    )\n",
    "                else:\n",
    "                    labels = torch.tensor(\n",
    "                        [f[\"labels\"] for f in features], dtype=torch.float\n",
    "                    )\n",
    "                batch = {\"labels\": labels}\n",
    "            for k, v in first.items():\n",
    "                if k != \"labels\" and v is not None and not isinstance(v, str):\n",
    "                    batch[k] = torch.stack([f[k] for f in features])\n",
    "            return batch\n",
    "        else:\n",
    "            # otherwise, revert to using the default collate_batch\n",
    "            return DefaultDataCollator().collate_batch(features)\n",
    "\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e1eb2c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class to load data with its task name. Decorator for changing Dataloader function to use a task name\n",
    "class DataLoaderTaskname:\n",
    "    def __init__(self, task, data_loader):\n",
    "        self.task=task\n",
    "        self.data_loader=data_loader\n",
    "        self.batch_size=data_loader.batch_size\n",
    "        self.dataset=data_loader.dataset\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_loader)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for batch in self.data_loader:\n",
    "            batch[\"task\"]=StrIgnoreDevice(self.task)\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b962376",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class to combine several data loaders into a single \"data loader\" \n",
    "class MultitaskDataLoader:\n",
    "    \n",
    "    def __init__(self, dataloader_dict):\n",
    "        self.dataloader_dict =dataloader_dict\n",
    "        self.num_batches_dict= {task:len(dataloader) for task, dataloader in self.dataloader_dict.items()}\n",
    "        self.task_lst        =list(self.dataloader_dict)\n",
    "        self.dataset = [None] * sum(len(dataloader.dataset) for dataloader in self.dataloader_dict.values())\n",
    "    def __len__(self):\n",
    "        return sum(self.num_batches_dict.values())\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        For each batch, sample a task, and get a batch from the respective task Dataloader.\n",
    "\n",
    "        We use size-proportional sampling, but you could easily modify this\n",
    "        to sample from some-other distribution.\n",
    "        \"\"\"\n",
    "        task_choice_list = []\n",
    "        for i, task in enumerate(self.task_lst):\n",
    "            task_choice_list += [i] * self.num_batches_dict[task]\n",
    "        task_choice_list = np.array(task_choice_list)\n",
    "        np.random.shuffle(task_choice_list)\n",
    "        dataloader_iter_dict = {\n",
    "            task: iter(dataloader) \n",
    "            for task, dataloader in self.dataloader_dict.items()\n",
    "        }\n",
    "        for task_choice in task_choice_list:\n",
    "            task = self.task_lst[task_choice]\n",
    "            yield next(dataloader_iter_dict[task]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc7a5242",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class to set up the trainer \n",
    "class MultitaskTrainer(transformers.Trainer):\n",
    "    \n",
    "    def single_task_dataloader(self,task,train_dataset):\n",
    "        \"\"\"\n",
    "        returns the single task data loader of a given task \n",
    "        \"\"\"\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer needs a dataset...:(\")\n",
    "        #if torch.cuda.is_available():#is_tpu_available():\n",
    "            #train_sampler=get_tpu_sampler(train_dataset)\n",
    "        else:\n",
    "            train_sampler=(RandomSampler(train_dataset)  if self.args.local_rank== -1 \n",
    "                                                            else DistributedSampler(train_dataset))\n",
    "    \n",
    "        data_loader=DataLoaderTaskname(task=task,data_loader=DataLoader(train_dataset,\n",
    "                                                                       batch_size=self.args.train_batch_size,\n",
    "                                                                       sampler=train_sampler,\n",
    "                                                                       collate_fn=self.data_collator.collate_batch\n",
    "                                                                      ))   \n",
    "        #if torch.cuda.is_available():#is_tpu_available():\n",
    "            #data_loader=pl.ParallelLoader(\n",
    "                #data_loader, [self.args.device]\n",
    "            #).per_device_loader(self.args.device)\n",
    "        return data_loader\n",
    "    \n",
    "    \n",
    "    def get_train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Returns a MultitaskDataLoader, which is not actually a Dataloader\n",
    "        but an iterable that returns a generator that samples from each \n",
    "        task Dataloader\n",
    "        \"\"\"\n",
    "        return MultitaskDataLoader({\n",
    "            task: self.single_task_dataloader(task, task_dataset)\n",
    "            for task, task_dataset in self.train_dataset.items()\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81478238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 408192\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 102050\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='102050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    10/102050 00:00 < 3:29:14, 8.13 it/s, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/pbs.4483716.sched01/ipykernel_77382/269490217.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1256\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1258\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m                 \u001b[0;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/tmp/pbs.4483716.sched01/ipykernel_77382/285789682.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtask_choice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtask_choice_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_lst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_choice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/tmp/pbs.4483716.sched01/ipykernel_77382/2052814330.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStrIgnoreDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/tmp/pbs.4483716.sched01/ipykernel_77382/2187151320.py\u001b[0m in \u001b[0;36mcollate_batch\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"labels\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                     \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"
     ]
    }
   ],
   "source": [
    "train_dataset = {\n",
    "    task: dataset[\"train\"] \n",
    "    for task, dataset in features_dict.items()\n",
    "}\n",
    "trainer = MultitaskTrainer(\n",
    "    model=multitask_model,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=\"./models/multitask_model\",\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=1e-5,\n",
    "        do_train=True,\n",
    "        num_train_epochs=2,\n",
    "        # Adjust batch size if this doesn't fit on the Colab GPU\n",
    "        per_device_train_batch_size=8,  \n",
    "        save_steps=3000,\n",
    "    ),\n",
    "    data_collator=NLPDataCollator(),\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d9e055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
